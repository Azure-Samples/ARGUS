{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/src/functionapp\")\n",
    "#On windows use  sys.path.append(module_path+\"\\\\src\\\\functionapp\")\n",
    "\n",
    "from ai_ocr.azure.openai_ops import load_image, get_size_of_base64_images\n",
    "from ai_ocr.azure.images import convert_pdf_into_image\n",
    "from ai_ocr.model import Config\n",
    "from ai_ocr.genai_operations import get_structured_data\n",
    "from ai_ocr.azure.doc_intelligence import get_ocr_results\n",
    "\n",
    "# from langchain_core.output_parsers.json import parse_json_markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Solution once on the demo to produce an output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_directory = '../demo/medical-dataset/'\n",
    "input_directory = '../demo/default-dataset/'\n",
    "#input_directory = '../demo/eval-dataset/'\n",
    "\n",
    "system_prompt =  ''\n",
    "with open(input_directory+'system_prompt.txt', 'r') as file_sys_prompt:\n",
    "    system_prompt = file_sys_prompt.read()\n",
    "\n",
    "output_schema = ''\n",
    "with open(input_directory+'output_schema.json', 'r') as file_output_schema:\n",
    "    output_schema = file_output_schema.read()\n",
    "\n",
    "# Create a dict with content key to store the OCR results\n",
    "ocr_result = {\n",
    "    \"content\": \"\"\n",
    "}\n",
    "\n",
    "# Loop over directory and process all PDFs\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        ocr_result[\"content\"] += get_ocr_results(input_directory+file).content\n",
    "\n",
    "        # Extract images from the PDF\n",
    "        convert_pdf_into_image(input_directory+file)\n",
    "    \n",
    " # Ensure the /tmp/ directory exists\n",
    "imgs_path = \"./tmp/\"\n",
    "os.makedirs(imgs_path, exist_ok=True)\n",
    "    \n",
    "# Determine the path for the temporary images\n",
    "imgs = glob.glob(f\"{imgs_path}/page*.png\")\n",
    "    \n",
    "# Limit images by config\n",
    "config = Config()\n",
    "print(f\"Config img size: {config.max_images}\")\n",
    "imgs = imgs[:config.max_images]\n",
    "imgs = [load_image(img) for img in imgs]\n",
    "print(f\"Images count: {len(imgs)}\")\n",
    "\n",
    "# Check and reduce images total size if over 20MB\n",
    "#max_size = 20 * 1024 * 1024  # 20MB\n",
    "#while get_size_of_base64_images(imgs) > max_size:\n",
    "#    imgs.pop()\n",
    "    \n",
    "# Get structured data\n",
    "structured = await get_structured_data(ocr_result[\"content\"], system_prompt, output_schema, imgs)\n",
    "    \n",
    "# Parse structured data and return as JSON\n",
    "# Parse structured data\n",
    "stripped_content = structured.content.strip()\n",
    "x = json.loads(stripped_content)\n",
    "response = json.dumps(x)\n",
    "\n",
    "print(f'Response: {response}')\n",
    "\n",
    "actual_output_path = \"./tmp/output.json\"\n",
    "with open(actual_output_path, 'w') as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating with the LLM\n",
    "\n",
    "### Load the input (an output from previous LLM run) and evaluate using LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent, ImageContent\n",
    "from ai_ocr.genai_operations import get_completion_service\n",
    "\n",
    "with open(actual_output_path, 'r') as f:\n",
    "    input = f.readlines()\n",
    "\n",
    "#print(input)\n",
    "\n",
    "system_message = f\"\"\"\n",
    "\n",
    "         You are the OpenAI model that can extract and describe images provided by the user in extreme detail. \n",
    "         The user attached images to this message for you to analyse, there is MOST DEFINITELY an image attached, you will never reply saying that you cannot see the image because the image is absolutely and always attached to this message.\n",
    "         \n",
    "         Your tasks are the following:\n",
    "         1. Use the images provided to extract all the data using this customs instructions:\n",
    "            ```\n",
    "            {system_prompt}\n",
    "            ```\n",
    "            and fit the data following the provided JSON schema:\n",
    "            ```\n",
    "            {output_schema}\n",
    "            ```\n",
    "            No need to output this extraction in your response.\n",
    "\n",
    "         2. Compare the data provided in the json input fields against what you have extracted from the images at task 1.\n",
    "\n",
    "            For each individual field in the json input, how confident you are that what you've extracted from the images field is accurate compared to the same field in the json input? Assign a confidence score between 0 and 1, using the following guidelines:\n",
    "            - 1.0: Perfect match, absolutely certain\n",
    "            - 0.9-0.99: Very high confidence, but not absolutely perfect\n",
    "            - 0.7-0.89: Good confidence, minor uncertainties\n",
    "            - 0.5-0.69: Moderate confidence, some discrepancies or uncertainties\n",
    "            - 0.3-0.49: Low confidence, significant discrepancies\n",
    "            - 0.1-0.29: Very low confidence, major discrepancies\n",
    "            - 0.0: Completely incorrect or unable to verify\n",
    " \n",
    "            Be critical in your evaluation. It's extremely rare for fields to have perfect confidence scores. If you're unsure about a field assign a lower confidence score.\n",
    "\n",
    "            Your final response is a json object like the following:\n",
    " \n",
    "            {{\n",
    "                \"field_name\": {{\n",
    "                    \"input_value\": input_value,\n",
    "                    \"image_value\": extracted_value,\n",
    "                    \"confidence\": confidence_score,\n",
    "                }},\n",
    "                ...\n",
    "            }}\n",
    "\n",
    "    \n",
    "         3. Create an object named \"accuracyStatistics\" at the end of the json that contains these 4 fields:\n",
    "          - \"numberOfFieldsSeenInImages\" is the total number of fields you extracted from the images (for all confidence score);\n",
    "          - \"numberofFieldsInSchema\"  is the total number of fields in the provided JSON schema (excluding parent objects for all confidence score);\n",
    "          - \"percentagePresenceAccuracy\" which is the ratio between the total fields in the provided JSON schema and the ones extracted from the images;\n",
    "          - \"overallFieldAccuracy\" is the sum of the confidence score you gave for each field in percentage.\n",
    "                  \n",
    "         ..and take your time to complete the tasks, don't rush.\n",
    "   \n",
    "        \"\"\"\n",
    "\n",
    "chat_history = ChatHistory(system_message=system_message)\n",
    "chat_history.add_user_message(f\"Here is the json input fields:\\n{input}\")\n",
    "\n",
    "if imgs:\n",
    "    chat_history.add_user_message(\"Here are the images from the document:\")\n",
    "    for img in imgs:\n",
    "        chat_history.add_message(\n",
    "            ChatMessageContent(\n",
    "                role=\"user\",\n",
    "                items=[ImageContent(uri=f\"data:image/png;base64,{img}\")]\n",
    "            )\n",
    "        )\n",
    "service, req_params = get_completion_service()\n",
    "req_params.extension_data[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "response = await service.get_chat_message_content(\n",
    "    chat_history,\n",
    "    req_params\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all generated images created after processing\n",
    "for file in os.listdir(imgs_path):\n",
    "    if file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "        image_path = os.path.join(imgs_path, file)\n",
    "        try:\n",
    "            os.remove(os.path.join(imgs_path, file))\n",
    "            print(f\"Deleted image: {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating with ground truth\n",
    "\n",
    "### Load the input (an output from previous LLM run), ground truth and create a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "def compile_jsonl(ground_truth_path, actual_output_path, output_file):\n",
    "    # Read the ground truth JSON file\n",
    "    with open(ground_truth_path, 'r') as gt_file:\n",
    "        ground_truth = json.load(gt_file)\n",
    "\n",
    "    with open(eval_schema_path, 'r') as eval_schema_file:\n",
    "        eval_schema = json.load(eval_schema_file)\n",
    "\n",
    "\n",
    "    # Open the output file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # Iterate over each actual output JSON file\n",
    "        with open(actual_output_path, 'r') as af:\n",
    "            actual_data = json.load(af)\n",
    "            # Combine ground truth and actual data into one object\n",
    "            combined_data = {\"ground_truth\": ground_truth, \"actual\": actual_data, \"eval_schema\":eval_schema}\n",
    "            # Write the combined data as a single line in the jsonl file\n",
    "            out_file.write(json.dumps(combined_data) + '\\n')\n",
    "\n",
    "\n",
    "ground_truth_path = f\"{module_path}/demo/default-dataset/ground_truth.json\"\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "eval_schema_path = f\"{module_path}/demo/default-dataset/evaluation_schema.json\"\n",
    "\n",
    "compile_jsonl(ground_truth_path, actual_output_path, eval_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "from src.evaluators.json_evaluator import JsonEvaluator\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "with open(eval_data_path) as file:\n",
    "    data = json.load(file)\n",
    "    ground_truth = data[\"ground_truth\"]\n",
    "    evaluation_schema = data[\"eval_schema\"]\n",
    "\n",
    "evaluators = {}\n",
    "evaluator_config = {}\n",
    "default_match_evaluator_config = {}\n",
    "json_evaluator = JsonEvaluator()\n",
    "evaluators[\"json_evaluator\"] = json_evaluator\n",
    "evaluator_config[\"json_evaluator\"] = {\n",
    "    \"actual\": \"${data.actual}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"eval_schema\": \"${data.eval_schema}\"\n",
    "}\n",
    "\n",
    "timestamp = time.strftime(\"%m_%d.%H.%M.%S\")\n",
    "output_path = f\"{module_path}/notebooks/outputs/output_{timestamp}.json\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"test_eval_1\",\n",
    "    data=eval_data_path,\n",
    "    evaluators=evaluators,\n",
    "    evaluator_config=evaluator_config,\n",
    "    output_path=output_path\n",
    ")\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orginize all results in output files in a dataframe and print as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "output_data_path = f\"{module_path}/notebooks/outputs/\"\n",
    "dfs = []\n",
    "merged_df = pd.DataFrame()\n",
    "run_number = 1\n",
    "\n",
    "filenames = sorted([f for f in os.listdir(output_data_path) if f.endswith(\".json\")])\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in filenames:\n",
    "    if filename.endswith(\".json\"):  # Check if the file is a JSON file\n",
    "        file_path = os.path.join(output_data_path, filename)\n",
    "        \n",
    "        # Load the JSON data\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Convert the 'metrics' dictionary to a DataFrame\n",
    "        df = pd.DataFrame.from_dict(data['metrics'], orient='index', columns=[filename])\n",
    "\n",
    "        df = df[df.index.str.startswith('json_evaluator.CustomStringEvaluator')]\n",
    "        df.reset_index(inplace=True)\n",
    "        df.columns = ['Fields', f'Run {run_number}']\n",
    "        \n",
    "                # Merge DataFrames\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on=\"Fields\", how='outer')\n",
    "        run_number += 1\n",
    "\n",
    "merged_df['Fields'] = merged_df['Fields'].str.replace('json_evaluator.CustomStringEvaluator.', '')\n",
    "merged_df['Average'] = merged_df.iloc[:, 1:].mean(axis=1)\n",
    "merged_df = merged_df.round(1)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataframe with seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4, 10))\n",
    "\n",
    "merged_df.set_index('Fields', inplace=True)\n",
    "\n",
    "sns.barplot(x=\"Average\", y=\"Fields\", data=merged_df, label=\"Fields\", color=\"b\")\n",
    "\n",
    "plt.figure(figsize=(4, 10))\n",
    "cmap = sns.color_palette([\"blue\", \"orange\"])\n",
    "sns.heatmap(merged_df.iloc[:, :-1], annot=True, cmap=cmap, cbar=False, linewidths=.5, fmt='g', annot_kws={\"size\": 10, \"color\": \"black\"})\n",
    "plt.title('Field Performance Across Runs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
