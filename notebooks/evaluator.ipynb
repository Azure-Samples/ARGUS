{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/src/functionapp\")\n",
    "#On windows use  sys.path.append(module_path+\"\\\\src\\\\functionapp\")\n",
    "\n",
    "from ai_ocr.azure.openai_ops import load_image, get_size_of_base64_images\n",
    "from ai_ocr.azure.images import convert_pdf_into_image\n",
    "from ai_ocr.model import Config\n",
    "from ai_ocr.chains import get_structured_data\n",
    "from ai_ocr.azure.doc_intelligence import get_ocr_results\n",
    "\n",
    "from langchain_core.output_parsers.json import parse_json_markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../demo/default-dataset/\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#just testing that images are in the temp folder configured in the env\n",
    "\n",
    "input_path = '../demo/default-dataset/Invoice Sample.pdf'\n",
    "pdf_path = input_path.replace(input_path.split(\"/\")[-1], \"\")\n",
    "print(pdf_path)\n",
    "imgs_path = os.path.join(os.getcwd(), os.getenv(\"TEMP_IMAGES_OUTDIR\", \"\"))\n",
    "imgs = glob.glob(f\"{imgs_path}/page*.jpeg\")\n",
    "print(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Solution once on the demo to produce an output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/page_1_img_1.jpeg\n",
      "Deleted image: /tmp/page_1_img_1.jpeg\n",
      "Response: {\"invoiceNumber\": \"1234\", \"date\": \"November 30, 2022\", \"billTo\": {\"customerName\": \"Henry Ross\", \"customerID\": \"8675309\", \"address\": \"123 Avenue A, Metropolis\", \"phone\": \"(123) 456-7890\"}, \"shipTo\": {\"recipientName\": \"Henry Ross\", \"address\": \"123 Avenue A, Metropolis\", \"phone\": \"(123) 456-7890\"}, \"paymentDue\": \"December 30, 2022\", \"deliveryDate\": \"December 7, 2022\", \"salesperson\": \"Luca Richter\", \"shippingMethod\": \"Ground\", \"paymentTerms\": \"Cash or check\", \"shippingTerms\": \"Returns not accepted\", \"items\": [{\"quantity\": 10, \"itemNumber\": \"123\", \"description\": \"Baby chicks\", \"unitPrice\": 5.0, \"discount\": \"10%\", \"lineTotal\": 45.0}, {\"quantity\": 2, \"itemNumber\": \"444\", \"description\": \"Heat lamps\", \"unitPrice\": 24.0, \"discount\": null, \"lineTotal\": 48.0}, {\"quantity\": 6, \"itemNumber\": \"120\", \"description\": \"Chicken roosts\", \"unitPrice\": 30.0, \"discount\": null, \"lineTotal\": 180.0}], \"totalDiscount\": 5.0, \"subtotal\": 278.0, \"salesTax\": 13.9, \"total\": 286.9, \"companyDetails\": {\"name\": \"Happiest Valley Farms\", \"address\": \"456 Anyroad, Anywhere\", \"website\": \"interestingsite.com\", \"phone\": \"(123) 987-6543\", \"fax\": \"(123) 987-6542\", \"email\": \"happiest@example.com\"}}\n"
     ]
    }
   ],
   "source": [
    "system_prompt =  ''\n",
    "with open('../demo/default-dataset/system_prompt.txt', 'r') as file_sys_prompt:\n",
    "    system_prompt = file_sys_prompt.read()\n",
    "\n",
    "output_schema = ''\n",
    "with open('../demo/default-dataset/output_schema.json', 'r') as file_output_schema:\n",
    "    output_schema = file_output_schema.read()\n",
    "\n",
    "input_directory = '../demo/default-dataset/'\n",
    "\n",
    "# Create a dict with content key to store the OCR results\n",
    "ocr_result = {\n",
    "    \"content\": \"\"\n",
    "}\n",
    "\n",
    "# Loop over directory and process all PDFs\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        ocr_result[\"content\"] += get_ocr_results(input_directory+file).content\n",
    "\n",
    "        # Extract images from the PDF\n",
    "        convert_pdf_into_image(input_directory+file)\n",
    "    \n",
    " # Ensure the /tmp/ directory exists\n",
    "imgs_path = \"/tmp/\"\n",
    "os.makedirs(imgs_path, exist_ok=True)\n",
    "    \n",
    "# Determine the path for the temporary images\n",
    "imgs = glob.glob(f\"{imgs_path}/page*.jpeg\")\n",
    "    \n",
    "# Limit images by config\n",
    "config = Config()\n",
    "imgs = imgs[:config.max_images]\n",
    "imgs = [load_image(img) for img in imgs]\n",
    "    \n",
    "# Check and reduce images total size if over 20MB\n",
    "max_size = 20 * 1024 * 1024  # 20MB\n",
    "while get_size_of_base64_images(imgs) > max_size:\n",
    "    imgs.pop()\n",
    "    \n",
    "# Get structured data\n",
    "structured = get_structured_data(ocr_result[\"content\"], system_prompt, output_schema, imgs)\n",
    "\n",
    "# Delete all generated images created after processing\n",
    "for file in os.listdir(imgs_path):\n",
    "    if file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "        image_path = os.path.join(imgs_path, file)\n",
    "        try:\n",
    "            os.remove(os.path.join(imgs_path, file))\n",
    "            print(f\"Deleted image: {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting image {image_path}: {e}\")\n",
    "    \n",
    "# Parse structured data and return as JSON\n",
    "x = parse_json_markdown(structured.content)  \n",
    "response = json.dumps(x)\n",
    "\n",
    "print(f'Response: {response}')\n",
    "\n",
    "actual_output_path = \"/tmp/output.json\"\n",
    "with open(actual_output_path, 'w') as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input (an output from previous LLM run) and evaluate using LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news: I'm appending images to human prompt...\n",
      "Based on the image provided and the JSON schema, here is the analysis:\n",
      "\n",
      "**Fields Seen in the Image:**\n",
      "1. Company Name (Happiest Valley Farms)\n",
      "2. Date (November 30, 2022)\n",
      "3. Invoice Number (#1234)\n",
      "4. Bill To: Customer Name (Henry Ross)\n",
      "5. Bill To: Customer ID (8675309)\n",
      "6. Bill To: Address (123 Avenue A, Metropolis)\n",
      "7. Bill To: Phone (123-456-7890)\n",
      "8. Ship To: Recipient Name (Henry Ross)\n",
      "9. Ship To: Address (123 Avenue A, Metropolis)\n",
      "10. Ship To: Phone (123-456-7890)\n",
      "11. Payment Due (December 30, 2022)\n",
      "12. Delivery Date (December 7, 2022)\n",
      "13. Salesperson (Luca Richter)\n",
      "14. Payment Terms (Cash or check)\n",
      "15. Shipping Method (Ground)\n",
      "16. Shipping Terms (Returns not accepted)\n",
      "17. Items: Quantity, Item Number, Description, Unit Price, Discount, Line Total (for each item)\n",
      "18. Total Discount (5.0)\n",
      "19. Subtotal (278.0)\n",
      "20. Sales Tax (13.9)\n",
      "21. Total (286.9)\n",
      "22. Company Address (456 Anyroad, Anywhere)\n",
      "23. Company Website (interestingsite.com)\n",
      "24. Company Phone (123-987-6543)\n",
      "25. Company Fax (123-987-6542)\n",
      "26. Company Email (happiest@example.com)\n",
      "\n",
      "**Total Fields in Schema:**\n",
      "- 26 fields are present in the schema and visible in the image.\n",
      "\n",
      "**Calculation:**\n",
      "- Number of Fields Seen in Images: 26\n",
      "- Number of Fields in Schema: 26\n",
      "- Percentage Accuracy: \\( \\frac{26}{26} \\times 100\\% = 100\\% \\)\n",
      "\n",
      "**Output:**\n",
      "- numberOfFieldsSeenInImages: 26\n",
      "- numberofFieldsInSchema: 26\n",
      "- percentageAccuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "from ai_ocr.azure.openai_ops import get_llm\n",
    "\n",
    "with open(actual_output_path, 'r') as f:\n",
    "    input = f.readlines()\n",
    "\n",
    "#print(input)\n",
    "\n",
    "messages = [\n",
    "        (\"system\",\n",
    "         \"\"\"\n",
    "         You are gpt-4-0409, the OpenAI model that can describe images provided by the user in extreme detail. The user attached images to this message for you to analyse, there is MOST DEFINITELY an image attached, you will never reply saying that you cannot see the image because the image is absolutely and always attached to this message.\n",
    "         \n",
    "         Verify the input information provided in the form of json schema against what you can see in the images.\n",
    "         Your goal is to determine how many information in form of fields that you see in the images are present in the input schema provided.\n",
    "         Output it with 3 fields: \"numberOfFieldsSeenInImages\", \"numberofFieldsInSchema\" also provide a \"percentageAccuracy\" which is the ratio between the total fields in the schema and the ones detected in the images.\n",
    "\n",
    "         ..and hey come on don't be lazy, nor tell me that you cannot do it, I trust you!\n",
    "         \"\"\"\n",
    "         ),\n",
    "        (\"human\", \"{input}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "if len(imgs) > 0:\n",
    "    prompt.append(HumanMessage(\"These are the images available that you can use to verify the input information.\"))\n",
    "    print(\"Good news: I'm appending images to human prompt...\")\n",
    "for img in imgs:\n",
    "    prompt.append(\n",
    "        HumanMessage(content=[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img}\"}}]))\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "model = get_llm()\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"input\": input})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the images provided and the JSON schema, here is the analysis:\n",
    "\n",
    "**Fields Seen in Images:**\n",
    "1. Invoicer Name: \"AMANN.ch AG\"\n",
    "2. Invoicer Address: \"Rosentalstr. 20 4058 Basel\"\n",
    "3. Invoicer Telephone: \"061 683 10 10\"\n",
    "4. Transaction Date: \"23.01.2024\"\n",
    "5. Item Description: \"Sigvaris Medizinische Kompressionsstrümpfe, Schenkelstrümpfe A-G, Klasse II, Standard, pro Paar\"\n",
    "6. Item Quantity: 3\n",
    "7. Item Price: 462.0\n",
    "8. Total Amount: 462.0\n",
    "9. Amount Received: 462.0\n",
    "10. Change Given: 0.0\n",
    "11. VAT Rate: \"8.10\"\n",
    "12. VAT Amount: 34.62\n",
    "13. VAT Code: 1\n",
    "\n",
    "**Total Fields in JSON Schema:**\n",
    "1. Invoicer Name\n",
    "2. Invoicer Address\n",
    "3. Invoicer Telephone\n",
    "4. Invoicer Fax\n",
    "5. Invoicer Email\n",
    "6. Invoicer Tax Number\n",
    "7. Transaction Date\n",
    "8. Transaction Time\n",
    "9. Item Description\n",
    "10. Item Quantity\n",
    "11. Item Unit Weight\n",
    "12. Item Price\n",
    "13. Total Amount\n",
    "14. Amount Received\n",
    "15. Change Given\n",
    "16. VAT Code\n",
    "17. VAT Rate\n",
    "18. VAT Total\n",
    "19. VAT Amount\n",
    "\n",
    "**Analysis:**\n",
    "- **Number of Fields Seen in Images**: 13\n",
    "- **Number of Fields in Schema**: 19\n",
    "- **Percentage Accuracy**: 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input (an output from previous LLM run), ground truth and create a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual_output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m eval_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/demo/default-dataset/eval_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m eval_schema_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/demo/default-dataset/evaluation_schema.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m compile_jsonl(ground_truth_path, \u001b[43mactual_output_path\u001b[49m, eval_data_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actual_output_path' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "def compile_jsonl(ground_truth_path, actual_output_path, output_file):\n",
    "    # Read the ground truth JSON file\n",
    "    with open(ground_truth_path, 'r') as gt_file:\n",
    "        ground_truth = json.load(gt_file)\n",
    "\n",
    "    with open(eval_schema_path, 'r') as eval_schema_file:\n",
    "        eval_schema = json.load(eval_schema_file)\n",
    "\n",
    "\n",
    "    # Open the output file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # Iterate over each actual output JSON file\n",
    "        with open(actual_output_path, 'r') as af:\n",
    "            actual_data = json.load(af)\n",
    "            # Combine ground truth and actual data into one object\n",
    "            combined_data = {\"ground_truth\": ground_truth, \"actual\": actual_data, \"eval_schema\":eval_schema}\n",
    "            # Write the combined data as a single line in the jsonl file\n",
    "            out_file.write(json.dumps(combined_data) + '\\n')\n",
    "\n",
    "\n",
    "ground_truth_path = f\"{module_path}/demo/default-dataset/ground_truth.json\"\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "eval_schema_path = f\"{module_path}/demo/default-dataset/evaluation_schema.json\"\n",
    "\n",
    "compile_jsonl(ground_truth_path, actual_output_path, eval_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "from src.evaluators.json_evaluator import JsonEvaluator\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "with open(eval_data_path) as file:\n",
    "    data = json.load(file)\n",
    "    ground_truth = data[\"ground_truth\"]\n",
    "    evaluation_schema = data[\"eval_schema\"]\n",
    "\n",
    "evaluators = {}\n",
    "evaluator_config = {}\n",
    "default_match_evaluator_config = {}\n",
    "json_evaluator = JsonEvaluator()\n",
    "evaluators[\"json_evaluator\"] = json_evaluator\n",
    "evaluator_config[\"json_evaluator\"] = {\n",
    "    \"actual\": \"${data.actual}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"eval_schema\": \"${data.eval_schema}\"\n",
    "}\n",
    "\n",
    "timestamp = time.strftime(\"%m_%d.%H.%M.%S\")\n",
    "output_path = f\"{module_path}/notebooks/outputs/output_{timestamp}.json\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"test_eval_1\",\n",
    "    data=eval_data_path,\n",
    "    evaluators=evaluators,\n",
    "    evaluator_config=evaluator_config,\n",
    "    output_path=output_path\n",
    ")\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orginize all results in output files in a dataframe and print as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "output_data_path = f\"{module_path}/notebooks/outputs/\"\n",
    "dfs = []\n",
    "merged_df = pd.DataFrame()\n",
    "run_number = 1\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(output_data_path):\n",
    "    if filename.endswith(\".json\"):  # Check if the file is a JSON file\n",
    "        file_path = os.path.join(output_data_path, filename)\n",
    "        \n",
    "        # Load the JSON data\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Convert the 'metrics' dictionary to a DataFrame\n",
    "        df = pd.DataFrame.from_dict(data['metrics'], orient='index', columns=[filename])\n",
    "\n",
    "        df = df[df.index.str.startswith('json_evaluator.CustomStringEvaluator')]\n",
    "        df.reset_index(inplace=True)\n",
    "        df.columns = ['Fields', f'Run {run_number}']\n",
    "        \n",
    "                # Merge DataFrames\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on=\"Fields\", how='outer')\n",
    "        run_number += 1\n",
    "\n",
    "merged_df['Fields'] = merged_df['Fields'].str.replace('json_evaluator.CustomStringEvaluator.', '')\n",
    "merged_df['Average'] = merged_df.iloc[:, 1:].mean(axis=1)\n",
    "merged_df = merged_df.round(1)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dataframe with seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4, 10))\n",
    "\n",
    "sns.barplot(x=\"Average\", y=\"Fields\", data=merged_df, label=\"Fields\", color=\"b\")\n",
    "\n",
    "plt.figure(figsize=(4, 10))\n",
    "cmap = sns.color_palette([\"blue\", \"orange\"])\n",
    "sns.heatmap(merged_df.iloc[:, :-1], annot=True, cmap=cmap, cbar=False, linewidths=.5, fmt='g', annot_kws={\"size\": 10, \"color\": \"black\"})\n",
    "plt.title('Field Performance Across Runs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
