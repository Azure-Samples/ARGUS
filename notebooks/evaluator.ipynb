{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/src/functionapp\")\n",
    "#On windows use  sys.path.append(module_path+\"\\\\src\\\\functionapp\")\n",
    "\n",
    "from ai_ocr.azure.openai_ops import load_image, get_size_of_base64_images\n",
    "from ai_ocr.azure.images import convert_pdf_into_image\n",
    "from ai_ocr.model import Config\n",
    "from ai_ocr.chains import get_structured_data\n",
    "from ai_ocr.azure.doc_intelligence import get_ocr_results\n",
    "\n",
    "from langchain_core.output_parsers.json import parse_json_markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just testing that images are in the temp folder configured in the env\n",
    "\n",
    "input_path = '../demo/default-dataset/Invoice Sample.pdf'\n",
    "#input_path = '../demo/evaltest-dataset/claim1.pdf'\n",
    "pdf_path = input_path.replace(input_path.split(\"/\")[-1], \"\")\n",
    "print(pdf_path)\n",
    "imgs_path = os.path.join(os.getcwd(), os.getenv(\"TEMP_IMAGES_OUTDIR\", \"\"))\n",
    "imgs = glob.glob(f\"{imgs_path}/page*.jpeg\")\n",
    "print(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Solution once on the demo to produce an output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =  ''\n",
    "with open('../demo/default-dataset/system_prompt.txt', 'r') as file_sys_prompt:\n",
    "    system_prompt = file_sys_prompt.read()\n",
    "\n",
    "output_schema = ''\n",
    "with open('../demo/default-dataset/output_schema.json', 'r') as file_output_schema:\n",
    "    output_schema = file_output_schema.read()\n",
    "\n",
    "input_directory = '../demo/default-dataset/'\n",
    "#input_directory = '../demo/evaltest-dataset/'\n",
    "\n",
    "# Create a dict with content key to store the OCR results\n",
    "ocr_result = {\n",
    "    \"content\": \"\"\n",
    "}\n",
    "\n",
    "# Loop over directory and process all PDFs\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        ocr_result[\"content\"] += get_ocr_results(input_directory+file).content\n",
    "\n",
    "        # Extract images from the PDF\n",
    "        convert_pdf_into_image(input_directory+file)\n",
    "    \n",
    " # Ensure the /tmp/ directory exists\n",
    "imgs_path = \"/tmp/\"\n",
    "os.makedirs(imgs_path, exist_ok=True)\n",
    "    \n",
    "# Determine the path for the temporary images\n",
    "imgs = glob.glob(f\"{imgs_path}/page*.png\")\n",
    "    \n",
    "# Limit images by config\n",
    "config = Config()\n",
    "print(f\"Config img size: {config.max_images}\")\n",
    "imgs = imgs[:config.max_images]\n",
    "imgs = [load_image(img) for img in imgs]\n",
    "print(f\"Images count: {len(imgs)}\")\n",
    "\n",
    "# Check and reduce images total size if over 20MB\n",
    "#max_size = 20 * 1024 * 1024  # 20MB\n",
    "#while get_size_of_base64_images(imgs) > max_size:\n",
    "#    imgs.pop()\n",
    "    \n",
    "# Get structured data\n",
    "structured = get_structured_data(ocr_result[\"content\"], system_prompt, output_schema, imgs)\n",
    "    \n",
    "# Parse structured data and return as JSON\n",
    "x = parse_json_markdown(structured.content)  \n",
    "response = json.dumps(x)\n",
    "\n",
    "print(f'Response: {response}')\n",
    "\n",
    "actual_output_path = \"/tmp/output.json\"\n",
    "with open(actual_output_path, 'w') as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input (an output from previous LLM run) and evaluate using LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "from ai_ocr.azure.openai_ops import get_llm\n",
    "\n",
    "with open(actual_output_path, 'r') as f:\n",
    "    input = f.readlines()\n",
    "\n",
    "#print(input)\n",
    "\n",
    "messages = [\n",
    "        (\"system\",\n",
    "         \"\"\"\n",
    "         You are the OpenAI model that can describe images provided by the user in extreme detail. \n",
    "         The user attached images to this message for you to analyse, there is MOST DEFINITELY an image attached, you will never reply saying that you cannot see the image because the image is absolutely and always attached to this message.\n",
    "         \n",
    "         Your tasks are the following:\n",
    "         1. Verify the data provided in the json input fields against what you can see in the images.\n",
    "         For each of the field in the input json give an accuracy score compared to the same field you see in the images (from 0 to 10 where 0 is wrong and 10 is '100%' correct).\n",
    "         In the images, there may be empty or not populated fields, you can ignore these, pay attention to boxes or checkboxes where the value to extract is usually marked with a cross or an 'X'.\n",
    "         Include in the response both the data extracted from the image compared to the one in the input and include the accuracy.\n",
    "         \n",
    "         2. Determine how many fields are present in the input providedcompared to the ones you see in the images.\n",
    "         Output it with 4 fields: \"numberOfFieldsSeenInImages\", \"numberofFieldsInSchema\" also provide a \"percentagePresenceAccuracy\" which is the ratio between the total fields in the schema and the ones detected in the images, the last field \"overallFieldAccuracy\" is the sum of the accuracy you gave for each field in percentage.\n",
    "         Don't include any other information in the response.\n",
    "                  \n",
    "         ..and take your time to complete the tasks.\n",
    "         \"\"\"\n",
    "         ),\n",
    "        (\"human\", \"{input}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "if len(imgs) > 0:\n",
    "    prompt.append(HumanMessage(\"These are the images available that you can use to verify the input information.\"))\n",
    "    print(\"Good news: I'm appending images to human prompt...\")\n",
    "for img in imgs:\n",
    "    prompt.append(\n",
    "        HumanMessage(content=[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img}\"}}]))\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "model = get_llm()\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"input\": input})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted image: /tmp/page_1.png\n"
     ]
    }
   ],
   "source": [
    "# Delete all generated images created after processing\n",
    "for file in os.listdir(imgs_path):\n",
    "    if file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "        image_path = os.path.join(imgs_path, file)\n",
    "        try:\n",
    "            os.remove(os.path.join(imgs_path, file))\n",
    "            print(f\"Deleted image: {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input (an output from previous LLM run), ground truth and create a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "def compile_jsonl(ground_truth_path, actual_output_path, output_file):\n",
    "    # Read the ground truth JSON file\n",
    "    with open(ground_truth_path, 'r') as gt_file:\n",
    "        ground_truth = json.load(gt_file)\n",
    "\n",
    "    with open(eval_schema_path, 'r') as eval_schema_file:\n",
    "        eval_schema = json.load(eval_schema_file)\n",
    "\n",
    "\n",
    "    def merge_dicts(ground_truth, eval_schema):\n",
    "        merged_dict = {}\n",
    "        \n",
    "        for key, value in ground_truth.items():\n",
    "            if isinstance(value, dict):\n",
    "                merged_dict[key] = merge_dicts(value, eval_schema.get(key, {}))\n",
    "            elif isinstance(value, list):\n",
    "                merged_dict[key] = {\n",
    "                    \"value\": value,\n",
    "                    \"eval\": eval_schema.get(key, [])\n",
    "                }\n",
    "            else:\n",
    "                merged_dict[key] = {\n",
    "                    \"value\": value,\n",
    "                    \"eval\": eval_schema.get(key, [])\n",
    "                }\n",
    "        \n",
    "        return merged_dict\n",
    "\n",
    "    # Open the output file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # Iterate over each actual output JSON file\n",
    "        with open(actual_output_path, 'r') as af:\n",
    "            actual_data = json.load(af)\n",
    "            # Combine ground truth and actual data into one object\n",
    "            combined_data = {\"ground_truth\": ground_truth, \"actual\": actual_data, \"eval_schema\":eval_schema}\n",
    "            # Write the combined data as a single line in the jsonl file\n",
    "            out_file.write(json.dumps(combined_data) + '\\n')\n",
    "\n",
    "\n",
    "ground_truth_path = f\"{module_path}/demo/default-dataset/ground_truth.json\"\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "eval_schema_path = f\"{module_path}/demo/default-dataset/evaluation_schema.json\"\n",
    "\n",
    "compile_jsonl(ground_truth_path, actual_output_path, eval_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "from src.evaluators.json_evaluator import JsonEvaluator\n",
    "eval_data_path = f\"{module_path}/demo/default-dataset/eval_data.jsonl\"\n",
    "with open(eval_data_path) as file:\n",
    "    data = json.load(file)\n",
    "    ground_truth = data[\"ground_truth\"]\n",
    "    evaluation_schema = data[\"eval_schema\"]\n",
    "\n",
    "evaluators = {}\n",
    "evaluator_config = {}\n",
    "default_match_evaluator_config = {}\n",
    "json_evaluator = JsonEvaluator()\n",
    "evaluators[\"json_evaluator\"] = json_evaluator\n",
    "evaluator_config[\"json_evaluator\"] = {\n",
    "    \"actual\": \"${data.actual}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"eval_schema\": \"${data.eval_schema}\"\n",
    "}\n",
    "\n",
    "timestamp = time.strftime(\"%m_%d.%H.%M.%S\")\n",
    "output_path = f\"{module_path}/notebooks/outputs/output_{timestamp}.json\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"test_eval_1\",\n",
    "    data=eval_data_path,\n",
    "    evaluators=evaluators,\n",
    "    evaluator_config=evaluator_config,\n",
    "    output_path=output_path\n",
    ")\n",
    "pprint(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
