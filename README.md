# ğŸ‘ï¸ ARGUS: The All-Seeing Document Intelligence Platform

<div align="center">

[![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com)
[![OpenAI](https://img.shields.io/badge/GPT--4-412991?style=for-the-badge&logo=openai&logoColor=white)](https://openai.com)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

*Named after Argus Panoptes, the mythological giant with a hundred eyesâ€”ARGUS never misses a detail in your documents.*

</div>

## ğŸš€ Transform Document Processing with AI Intelligence

**ARGUS** revolutionizes how organizations extract, understand, and act on document data. By combining the precision of **Azure Document Intelligence** with the contextual reasoning of **GPT-4 Vision**, ARGUS doesn't just read documentsâ€”it *understands* them.

### ğŸ’¡ Why ARGUS?

Traditional OCR solutions extract text but miss the context. AI-only approaches struggle with complex layouts. **ARGUS bridges this gap**, delivering enterprise-grade document intelligence that:

- **ğŸ¯ Extracts with Purpose**: Understands document context, not just text
- **âš¡ Scales Effortlessly**: Process thousands of documents with cloud-native architecture
- **ğŸ”’ Secures by Design**: Enterprise security with managed identities and RBAC
- **ğŸ§  Learns Continuously**: Configurable datasets adapt to your specific document types
- **ğŸ“Š Measures Success**: Built-in evaluation tools ensure consistent accuracy

---

## ğŸŒŸ Key Capabilities

<table>
<tr>
<td width="50%">

### ğŸ” **Intelligent Document Understanding**
- **Hybrid AI Pipeline**: Combines OCR precision with LLM reasoning
- **Multiple OCR Providers**: Azure Document Intelligence or Mistral Document AI
- **Context-Aware Extraction**: Understands relationships between data points
- **Multi-Format Support**: PDFs, images, forms, invoices, medical records
- **Zero-Shot Learning**: Works on new document types without training

### âš¡ **Enterprise-Ready Performance**
- **Cloud-Native Architecture**: Built on Azure Container Apps
- **Scalable Processing**: Handle document floods with confidence
- **Real-Time Processing**: API-driven workflows for immediate results
- **Event-Driven Automation**: Automatic processing on document upload

</td>
<td width="50%">

### ğŸ›ï¸ **Advanced Control & Customization**
- **Dynamic Configuration**: Runtime settings without redeployment
- **Custom Datasets**: Tailor extraction for your specific needs
- **Interactive Chat**: Ask questions about processed documents
- **Concurrency Management**: Fine-tune performance for your workload

### ğŸ“ˆ **Comprehensive Analytics**
- **Built-in Evaluation**: Multiple accuracy metrics and comparisons
- **Performance Monitoring**: Application Insights integration
- **Custom Evaluators**: Fuzzy matching, semantic similarity, and more
- **Visual Analytics**: Jupyter notebooks for deep analysis

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture: Built for Scale and Security

ARGUS employs a modern, cloud-native architecture designed for enterprise workloads:

<div align="center">

```mermaid
graph TB
    subgraph "ğŸ“¥ Document Input"
        A[ğŸ“„ Documents] --> B[ğŸ“ Azure Blob Storage]
        C[ğŸŒ Direct Upload API] --> D[ğŸš€ FastAPI Backend]
    end
    
    subgraph "ğŸ§  AI Processing Engine"
        B --> D
        D --> E{ğŸ” OCR Provider}
        E -->|Azure| E1[Azure Document Intelligence]
        E -->|Mistral| E2[Mistral Document AI]
        D --> F[ğŸ¤– GPT-4 Vision]
        E1 --> G[âš™ï¸ Hybrid Processing Pipeline]
        E2 --> G
        F --> G
    end
    
    subgraph "ğŸ’¡ Intelligence & Analytics"
        G --> H[ğŸ“Š Custom Evaluators]
        G --> I[ğŸ’¬ Interactive Chat]
        H --> J[ğŸ“ˆ Results & Analytics]
    end
    
    subgraph "ğŸ’¾ Data Layer"
        G --> K[ğŸ—„ï¸ Azure Cosmos DB]
        J --> K
        I --> K
        K --> L[ğŸ“± Streamlit Frontend]
    end
    
    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style E1 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style E2 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style F fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    style G fill:#fff8e1,stroke:#ffa000,stroke-width:2px
    style H fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    style I fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    style J fill:#fdf2e9,stroke:#e65100,stroke-width:2px
    style K fill:#e0f7fa,stroke:#0097a7,stroke-width:2px
    style L fill:#f9fbe7,stroke:#827717,stroke-width:2px
```

</div>

### ğŸ”§ Infrastructure Components

| Component | Technology | Purpose |
|-----------|------------|---------|
| **ğŸš€ Backend API** | Azure Container Apps + FastAPI | High-performance document processing engine |
| **ğŸ“± Frontend UI** | Streamlit (Optional) | Interactive document management interface |
| **ğŸ“ Document Storage** | Azure Blob Storage | Secure, scalable document repository |
| **ğŸ—„ï¸ Metadata Database** | Azure Cosmos DB | Results, configurations, and analytics |
| **ğŸ” OCR Engine** | Azure Document Intelligence or Mistral Document AI | Structured text and layout extraction |
| **ğŸ§  AI Reasoning** | Azure OpenAI (GPT-4 Vision) | Contextual understanding and extraction |
| **ğŸ—ï¸ Container Registry** | Azure Container Registry | Private, secure container images |
| **ğŸ”’ Security** | Managed Identity + RBAC | Zero-credential architecture |
| **ğŸ“Š Monitoring** | Application Insights | Performance and health monitoring |

---

## âš¡ Quick Start: Deploy in Minutes

### ğŸ“‹ Prerequisites

<details>
<summary><b>ğŸ› ï¸ Required Tools (Click to expand)</b></summary>

1. **Docker**
   ```bash
   # Install Docker (required for containerization during deployment)
   # Visit https://docs.docker.com/get-docker/ for installation instructions
   ```

2. **Azure Developer CLI (azd)**
   ```bash
   curl -fsSL https://aka.ms/install-azd.sh | bash
   ```

3. **Azure CLI**
   ```bash
   curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
   ```

4. **Azure OpenAI Resource** 
   - Create an Azure OpenAI resource in a [supported region](https://docs.microsoft.com/azure/cognitive-services/openai/overview#regional-availability)
   - Deploy a vision-capable model: `gpt-4o`, `gpt-4-turbo`, or `gpt-4` (with vision)
   - Collect: endpoint URL, API key, and deployment name

</details>

### ğŸš€ One-Command Deployment

```bash
# 1. Clone the repository
git clone https://github.com/Azure-Samples/ARGUS.git
cd ARGUS

# 2. Login to Azure
az login

# 3. Deploy everything with a single command
azd up
```

**That's it!** ğŸ‰ Your ARGUS instance is now running in the cloud.

### âœ… Verify Your Deployment

```bash
# Check system health
curl "$(azd env get-value BACKEND_URL)/health"

# Expected response:
{
  "status": "healthy",
  "services": {
    "cosmos_db": "âœ… connected",
    "blob_storage": "âœ… connected", 
    "document_intelligence": "âœ… connected",
    "azure_openai": "âœ… connected"
  }
}

# View live application logs
azd logs --follow
```

---

## ğŸ® Usage Examples: See ARGUS in Action

### ğŸ“„ Method 1: Upload via Frontend Interface (Recommended)

The easiest way to process documents is through the user-friendly web interface:

1. **Access the Frontend**:
   ```bash
   # Get the frontend URL after deployment
   azd env get-value FRONTEND_URL
   ```

2. **Upload and Process Documents**:
   - Navigate to the **"ğŸ§  Process Files"** tab
   - Select your dataset from the dropdown (e.g., "default-dataset", "medical-dataset")
   - Use the **file uploader** to select PDF, image, or Office documents
   - Click **"Submit"** to upload files
   - Files are automatically processed using the selected dataset's configuration
   - Monitor processing status in the **"ğŸ” Explore Data"** tab

### ğŸ“¤ Method 2: Direct Blob Storage Upload

For automation or bulk processing, upload files directly to Azure Blob Storage:

```bash
# Upload a document to be processed automatically
az storage blob upload \
  --account-name "$(azd env get-value STORAGE_ACCOUNT_NAME)" \
  --container-name "datasets" \
  --name "default-dataset/invoice-2024.pdf" \
  --file "./my-invoice.pdf" \
  --auth-mode login

# Files uploaded to blob storage are automatically detected and processed
# Results can be viewed in the frontend or retrieved via API
```

### ğŸ’¬ Example 3: Interactive Document Chat

Ask questions about any processed document through the API:

```bash
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{
    "blob_url": "https://mystorage.blob.core.windows.net/datasets/default-dataset/contract.pdf",
    "question": "What are the key terms and conditions in this contract?"
  }' \
  "$(azd env get-value BACKEND_URL)/api/chat"

# Get intelligent answers:
{
  "answer": "The key terms include: 1) 12-month service agreement, 2) $5000/month fee, 3) 30-day termination clause...",
  "confidence": 0.91,
  "sources": ["page 1, paragraph 3", "page 2, section 2.1"]
}
```

---

## ğŸ¤– MCP Integration: AI-Powered Document Access

ARGUS supports the **Model Context Protocol (MCP)** using the modern **Streamable HTTP transport**, enabling AI assistants like GitHub Copilot, Claude, and other MCP-compatible clients to interact directly with your document intelligence platform.

### ğŸ”Œ What is MCP?

The [Model Context Protocol](https://modelcontextprotocol.io/) is an open standard that allows AI assistants to securely connect to external data sources and tools. With ARGUS MCP support, your AI assistant can:

- ğŸ“„ **List and search documents** across all your datasets
- ğŸ” **Query document content** and extracted data
- ğŸ’¬ **Chat with documents** using natural language
- ğŸ“¤ **Upload new documents** for processing
- âš™ï¸ **Manage datasets** and configurations

### âš¡ Quick Setup

Add ARGUS to your MCP client configuration:

**VS Code / GitHub Copilot** (`~/.vscode/mcp.json` or workspace settings):
```json
{
  "mcpServers": {
    "argus": {
      "url": "https://<your-backend-url>/mcp"
    }
  }
}
```

> **Tip**: After deployment with `azd up`, get your backend URL from the Azure Portal or run `azd show` to find the Container App URL.

**Claude Desktop** (`~/Library/Application Support/Claude/claude_desktop_config.json`):
```json
{
  "mcpServers": {
    "argus": {
      "url": "https://<your-backend-url>/mcp"
    }
  }
}
```

> **Note**: ARGUS uses the Streamable HTTP transport (the modern MCP standard). The endpoint is a single `/mcp` path that handles all MCP communication.

### ğŸ› ï¸ Available MCP Tools

| Tool | Description |
|------|-------------|
| `argus_list_documents` | List all processed documents with filtering options |
| `argus_get_document` | Get detailed document information including OCR and extraction results |
| `argus_chat_with_document` | Ask natural language questions about a document |
| `argus_search_documents` | Search documents by keyword across all datasets |
| `argus_list_datasets` | List available dataset configurations |
| `argus_get_dataset_config` | Get system prompt and schema for a dataset |
| `argus_create_dataset` | Create a new dataset with custom prompt and schema |
| `argus_process_document_url` | Queue a document for processing from blob URL |
| `argus_get_extraction` | Get extracted structured data from a document |
| `argus_get_upload_url` | Get a pre-signed SAS URL for direct document upload |

### ğŸ’¡ Example Interactions

Once configured, you can interact with ARGUS through your AI assistant:

```
User: "Show me all invoices processed in the last week"
AI: [Uses argus_list_documents to retrieve recent invoices]

User: "What's the total amount on invoice INV-2024-001?"
AI: [Uses argus_get_document to fetch invoice details]

User: "I need to upload a new contract for processing"
AI: [Uses argus_get_upload_url to get a secure upload link]

User: "Compare the extraction results between these two invoices"
AI: [Uses argus_get_extraction on both documents and compares]

User: "Create a new dataset for processing purchase orders"
AI: [Uses argus_create_dataset with appropriate prompt and schema]
```


---

## ğŸ›ï¸ Advanced Configuration

### ğŸ“Š Dataset Management

ARGUS uses **datasets** to define how different types of documents should be processed. A dataset contains:
- **Model Prompt**: Instructions telling the AI how to extract data from documents
- **Output Schema**: The target structure for extracted data (can be empty to let AI determine the structure)
- **Processing Options**: Settings for OCR, image analysis, summarization, and evaluation

**When to create custom datasets**: Create a new dataset when you have a specific document type that requires different extraction logic than the built-in datasets (e.g., contracts, medical reports, financial statements).

<details>
<summary><b>ğŸ—‚ï¸ Built-in Datasets</b></summary>

- **`default-dataset/`**: Invoices, receipts, general business documents
- **`medical-dataset/`**: Medical forms, prescriptions, healthcare documents

</details>

<details>
<summary><b>ğŸ”§ Create Custom Datasets</b></summary>

Datasets are managed through the Streamlit frontend interface (deployed automatically with azd):

1. **Access the frontend** (URL provided after azd deployment)
2. **Navigate to the Process Files tab**
3. **Scroll to "Add New Dataset" section**
4. **Configure your dataset**:
   - Enter dataset name (e.g., "legal-contracts")
   - Define model prompt with extraction instructions
   - Specify output schema (JSON format) or leave empty
   - Set processing options (OCR, images, evaluation)
5. **Click "Add New Dataset"** - it's saved directly to Cosmos DB

</details>

---

### ï¿½ OCR Provider Configuration

ARGUS supports **two OCR providers** for document text extraction:

- **Azure Document Intelligence** (Default): Microsoft's enterprise OCR service with advanced layout understanding
- **Mistral Document AI**: Mistral's document processing service with markdown-optimized output

<details>
<summary><b>ğŸ”§ Configure OCR Provider</b></summary>

**Via Frontend (Recommended)**:
1. Navigate to **Settings** tab in the web interface
2. Select **OCR Provider** section
3. Choose your provider:
   - **Azure**: Uses Azure Document Intelligence (automatically configured during deployment)
   - **Mistral**: Requires additional configuration (endpoint, API key, model name)
4. For Mistral, enter:
   - **Mistral Endpoint**: Your Mistral Document AI API endpoint URL
   - **Mistral API Key**: Your Mistral API authentication key
   - **Mistral Model**: Model name (default: `mistral-document-ai-2505`)
5. Click **"Update OCR Provider"** to apply changes

**Via Environment Variables**:
Set the following environment variables in your deployment:

```bash
# Choose OCR provider
OCR_PROVIDER=mistral  # or "azure" (default)

# Mistral-specific configuration (only needed if OCR_PROVIDER=mistral)
MISTRAL_DOC_AI_ENDPOINT=https://your-endpoint.services.ai.azure.com/providers/mistral/azure/ocr
MISTRAL_DOC_AI_KEY=your-mistral-api-key
MISTRAL_DOC_AI_MODEL=mistral-document-ai-2505
```

**Update via Azure Portal**:
1. Navigate to Azure Portal â†’ Container Apps â†’ Your Backend App
2. Go to **Settings** â†’ **Environment variables**
3. Add/update the variables listed above
4. **Restart** the container app

**Update via Azure CLI**:
```bash
# Switch to Mistral
az containerapp update \
  --name <your-backend-app-name> \
  --resource-group <your-resource-group> \
  --set-env-vars \
    OCR_PROVIDER="mistral" \
    MISTRAL_DOC_AI_ENDPOINT="https://your-endpoint.../ocr" \
    MISTRAL_DOC_AI_KEY="your-api-key" \
    MISTRAL_DOC_AI_MODEL="mistral-document-ai-2505"

# Switch back to Azure
az containerapp update \
  --name <your-backend-app-name> \
  --resource-group <your-resource-group> \
  --set-env-vars OCR_PROVIDER="azure"
```

**Note**: OCR provider selection is configured at the solution level and applies to all document processing operations.

</details>

---

The Streamlit frontend is **automatically deployed** with `azd up` and provides a user-friendly interface for document management.

<div align="center">
<img src="docs/ArchitectureOverview.png" alt="ARGUS Frontend Interface" width="800"/>
</div>

### ğŸ¯ Frontend Features

| Tab | Functionality |
|-----|---------------|
| **ğŸ§  Process Files** | Drag-and-drop document upload with real-time processing status |
| **ğŸ” Explore Data** | Browse processed documents, search results, view extraction details |
| **âš™ï¸ Settings** | Configure datasets, adjust processing parameters, manage connections |
| **ğŸ“‹ Instructions** | Interactive help, API documentation, and usage examples |

---

## ï¸ Development & Customization

### ğŸ—ï¸ Project Structure Deep Dive

```
ARGUS/
â”œâ”€â”€ ğŸ“‹ azure.yaml                        # Azure Developer CLI configuration
â”œâ”€â”€ ğŸ“„ README.md                         # Project documentation & setup guide
â”œâ”€â”€ ğŸ“„ LICENSE                           # MIT license file
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md                   # Contribution guidelines
â”œâ”€â”€ ğŸ“„ sample-invoice.pdf                # Sample document for testing
â”œâ”€â”€ ğŸ”§ .env.template                     # Environment variables template
â”œâ”€â”€ ğŸ“‚ .github/                          # GitHub Actions & workflows
â”œâ”€â”€ ğŸ“‚ .devcontainer/                    # Development container configuration
â”œâ”€â”€ ğŸ“‚ .vscode/                          # VS Code settings & extensions
â”‚
â”œâ”€â”€ ğŸ“‚ infra/                            # ğŸ—ï¸ Azure Infrastructure as Code
â”‚   â”œâ”€â”€ âš™ï¸ main.bicep                    # Primary Bicep template for Azure resources
â”‚   â”œâ”€â”€ âš™ï¸ main.parameters.json          # Infrastructure parameters & configuration
â”‚   â”œâ”€â”€ âš™ï¸ main-containerapp.bicep       # Container App specific infrastructure
â”‚   â”œâ”€â”€ âš™ï¸ main-containerapp.parameters.json # Container App parameters
â”‚   â””â”€â”€ ğŸ“‹ abbreviations.json            # Azure resource naming abbreviations
â”‚
â”œâ”€â”€ ğŸ“‚ src/                              # ğŸš€ Core Application Source Code
â”‚   â”œâ”€â”€ ğŸ“‚ containerapp/                 # FastAPI Backend Service
â”‚   â”‚   â”œâ”€â”€ ğŸš€ main.py                   # FastAPI app lifecycle & configuration
â”‚   â”‚   â”œâ”€â”€ ğŸ”Œ api_routes.py             # HTTP endpoints & request handlers
â”‚   â”‚   â”œâ”€â”€ ğŸ”§ dependencies.py           # Azure client initialization & management
â”‚   â”‚   â”œâ”€â”€ ğŸ“‹ models.py                 # Pydantic data models & schemas
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ blob_processing.py        # Document processing pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸ logic_app_manager.py     # Azure Logic Apps concurrency management
â”‚   â”‚   â”œâ”€â”€ ğŸ³ Dockerfile                # Container image definition
â”‚   â”‚   â”œâ”€â”€ ğŸ“¦ requirements.txt          # Python dependencies
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ REFACTORING_SUMMARY.md    # Architecture documentation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ ai_ocr/                   # ğŸ§  AI Processing Engine
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ” process.py            # Main processing orchestration & workflow
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ”— chains.py             # LangChain integration & AI workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¤– model.py              # Configuration models & data structures
â”‚   â”‚   â”‚   â”œâ”€â”€ â±ï¸ timeout.py            # Processing timeout management
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“‚ azure/                # â˜ï¸ Azure Service Integrations
â”‚   â”‚   â”‚       â”œâ”€â”€ âš™ï¸ config.py         # Environment & configuration management
â”‚   â”‚   â”‚       â”œâ”€â”€ ğŸ“„ doc_intelligence.py # Azure Document Intelligence OCR
â”‚   â”‚   â”‚       â”œâ”€â”€ ğŸ–¼ï¸ images.py         # PDF to image conversion utilities
â”‚   â”‚   â”‚       â””â”€â”€ ğŸ¤– openai_ops.py     # Azure OpenAI API operations
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ example-datasets/         # ğŸ“Š Default Dataset Configurations
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ datasets/                 # ğŸ“ Runtime dataset storage
â”‚   â”‚   â””â”€â”€ ğŸ“‚ evaluators/               # ğŸ“ˆ Data quality evaluation modules
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ evaluators/                   # ğŸ§ª Evaluation Framework
â”‚       â”œâ”€â”€ ğŸ“‹ field_evaluator_base.py   # Abstract base class for evaluators
â”‚       â”œâ”€â”€ ğŸ”¤ fuzz_string_evaluator.py  # Fuzzy string matching evaluation
â”‚       â”œâ”€â”€ ğŸ¯ cosine_similarity_string_evaluator.py # Semantic similarity evaluation
â”‚       â”œâ”€â”€ ğŸ›ï¸ custom_string_evaluator.py # Custom evaluation logic
â”‚       â”œâ”€â”€ ğŸ“Š json_evaluator.py         # JSON structure validation
â”‚       â””â”€â”€ ğŸ“‚ tests/                    # Unit tests for evaluators
â”‚
â”œâ”€â”€ ğŸ“‚ frontend/                         # ğŸ–¥ï¸ Streamlit Web Interface
â”‚   â”œâ”€â”€ ğŸ“± app.py                        # Main Streamlit application entry point
â”‚   â”œâ”€â”€ ğŸ”„ backend_client.py             # API client for backend communication
â”‚   â”œâ”€â”€ ğŸ“¤ process_files.py              # File upload & processing interface
â”‚   â”œâ”€â”€ ğŸ” explore_data.py               # Document browsing & analysis UI
â”‚   â”œâ”€â”€ ğŸ’¬ document_chat.py              # Interactive document Q&A interface
â”‚   â”œâ”€â”€ ğŸ“‹ instructions.py               # Help & documentation tab
â”‚   â”œâ”€â”€ âš™ï¸ settings.py                   # Configuration management UI
â”‚   â”œâ”€â”€ ğŸ›ï¸ concurrency_management.py    # Performance tuning interface
â”‚   â”œâ”€â”€ ğŸ“Š concurrency_settings.py      # Concurrency configuration utilities
â”‚   â”œâ”€â”€ ğŸ³ Dockerfile                    # Frontend container definition
â”‚   â”œâ”€â”€ ğŸ“¦ requirements.txt              # Python dependencies for frontend
â”‚   â””â”€â”€ ğŸ“‚ static/                       # Static assets (logos, images)
â”‚       â””â”€â”€ ğŸ–¼ï¸ logo.png                  # ARGUS brand logo
â”‚
â”œâ”€â”€ ğŸ“‚ demo/                             # ğŸ“‹ Sample Datasets & Examples
â”‚   â”œâ”€â”€ ğŸ“‚ default-dataset/              # General business documents dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ system_prompt.txt         # AI extraction instructions
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š output_schema.json        # Expected data structure
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ground_truth.json         # Validation reference data
â”‚   â”‚   â””â”€â”€ ğŸ“„ Invoice Sample.pdf        # Sample document for testing
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ medical-dataset/              # Healthcare documents dataset
â”‚       â”œâ”€â”€ ğŸ“„ system_prompt.txt         # Medical-specific extraction rules
â”‚       â”œâ”€â”€ ğŸ“Š output_schema.json        # Medical data structure
â”‚       â””â”€â”€ ğŸ“„ eyes_surgery_pre_1_4.pdf  # Sample medical document
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                        # ğŸ“ˆ Analytics & Evaluation Tools
â”‚   â”œâ”€â”€ ğŸ§ª evaluator.ipynb              # Comprehensive evaluation dashboard
â”‚   â”œâ”€â”€ ğŸ“Š output.json                  # Evaluation results & metrics
â”‚   â”œâ”€â”€ ğŸ“¦ requirements.txt              # Jupyter notebook dependencies
â”‚   â”œâ”€â”€ ğŸ“„ README.md                     # Notebook usage instructions
â”‚   â””â”€â”€ ğŸ“‚ outputs/                      # Historical evaluation results
â”‚
â””â”€â”€ ğŸ“‚ docs/                             # ğŸ“š Documentation & Assets
    â””â”€â”€ ğŸ–¼ï¸ ArchitectureOverview.png      # System architecture diagram
```

### ğŸ§ª Local Development Setup

```bash
# Setup development environment
cd src/containerapp
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt

# Configure local environment
cp ../../.env.template .env
# Edit .env with your development credentials

# Run with hot reload
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Access API documentation
open http://localhost:8000/docs
```

### ğŸ”§ Key Technologies & Libraries

| Category | Technologies |
|----------|-------------|
| **ğŸš€ API Framework** | FastAPI, Uvicorn, Pydantic |
| **ğŸ§  AI/ML** | LangChain, OpenAI SDK, Azure AI SDK |
| **â˜ï¸ Azure Services** | Azure SDK (Blob, Cosmos, Document Intelligence) |
| **ğŸ“„ Document Processing** | PyMuPDF, Pillow, PyPDF2 |
| **ğŸ“Š Data & Analytics** | Pandas, NumPy, Matplotlib |
| **ğŸ”’ Security** | Azure Identity, managed identities |

---

##  API Reference: Complete Documentation

### ğŸš€ Core Processing Endpoints

<details>
<summary><b>ğŸ“„ POST /api/process-blob - Process Document from Storage</b></summary>

**Request**:
```json
{
  "blob_url": "https://storage.blob.core.windows.net/datasets/default-dataset/invoice.pdf",
  "dataset_name": "default-dataset",
  "priority": "normal",
  "webhook_url": "https://your-app.com/webhooks/argus",
  "metadata": {
    "source": "email_attachment",
    "user_id": "user123"
  }
}
```

**Response**:
```json
{
  "status": "success",
  "job_id": "job_12345",
  "extraction_results": {
    "invoice_number": "INV-2024-001",
    "total_amount": "$1,250.00",
    "confidence_score": 0.94
  },
  "processing_time": "2.3s",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

</details>

<details>
<summary><b>ğŸ“¤ POST /api/process-file - Direct File Upload</b></summary>

**Request** (multipart/form-data):
```
file: [PDF/Image file]
dataset_name: "default-dataset"
priority: "high"
```

**Response**:
```json
{
  "status": "success",
  "job_id": "job_12346",
  "blob_url": "https://storage.blob.core.windows.net/temp/uploaded_file.pdf",
  "extraction_results": {...},
  "processing_time": "1.8s"
}
```

</details>

<details>
<summary><b>ğŸ’¬ POST /api/chat - Interactive Document Q&A</b></summary>

**Request**:
```json
{
  "blob_url": "https://storage.blob.core.windows.net/datasets/contract.pdf",
  "question": "What are the payment terms and penalties for late payment?",
  "context": "focus on financial obligations",
  "temperature": 0.1
}
```

**Response**:
```json
{
  "answer": "Payment terms are Net 30 days. Late payment penalty is 1.5% per month on outstanding balance...",
  "confidence": 0.91,
  "sources": [
    {"page": 2, "section": "Payment Terms"},
    {"page": 5, "section": "Default Provisions"}
  ],
  "processing_time": "1.2s"
}
```

</details>

### âš™ï¸ Configuration Management

<details>
<summary><b>ğŸ”§ GET/POST /api/configuration - System Configuration</b></summary>

**GET Response**:
```json
{
  "openai_settings": {
    "endpoint": "https://your-openai.openai.azure.com/",
    "model": "gpt-4o",
    "temperature": 0.1,
    "max_tokens": 4000
  },
  "processing_settings": {
    "max_concurrent_jobs": 5,
    "timeout_seconds": 300,
    "retry_attempts": 3
  },
  "datasets": ["default-dataset", "medical-dataset", "financial-reports"]
}
```

**POST Request**:
```json
{
  "openai_settings": {
    "temperature": 0.05,
    "max_tokens": 6000
  },
  "processing_settings": {
    "max_concurrent_jobs": 8
  }
}
```

</details>

### ğŸ“Š Monitoring & Analytics

<details>
<summary><b>ğŸ“ˆ GET /api/metrics - Performance Metrics</b></summary>

**Response**:
```json
{
  "period": "last_24h",
  "summary": {
    "total_documents": 1247,
    "successful_extractions": 1198,
    "failed_extractions": 49,
    "success_rate": 96.1,
    "avg_processing_time": "2.3s"
  },
  "performance": {
    "p50_processing_time": "1.8s",
    "p95_processing_time": "4.2s",
    "p99_processing_time": "8.1s"
  },
  "errors": {
    "ocr_failures": 12,
    "ai_timeouts": 8,
    "storage_issues": 3,
    "other": 26
  }
}
```

</details>

---

##  Contributing & Community

### ğŸ¯ How to Contribute

We welcome contributions! Here's how to get started:

1. **ğŸ´ Fork & Clone**:
   ```bash
   git clone https://github.com/your-username/ARGUS.git
   cd ARGUS
   ```

2. **ğŸŒ¿ Create Feature Branch**:
   ```bash
   git checkout -b feature/amazing-improvement
   ```

3. **ğŸ§ª Develop & Test**:
   ```bash
   # Setup development environment
   ./scripts/setup-dev.sh
   
   # Run tests
   pytest tests/ -v
   
   # Lint code
   black src/ && flake8 src/
   ```

4. **ğŸ“ Document Changes**:
   ```bash
   # Update documentation
   # Add examples to README
   # Update API documentation
   ```

5. **ğŸš€ Submit PR**:
   ```bash
   git commit -m "feat: add amazing improvement"
   git push origin feature/amazing-improvement
   # Create pull request on GitHub
   ```

### ğŸ“‹ Contribution Guidelines

| Type | Guidelines |
|------|------------|
| **ğŸ› Bug Fixes** | Include reproduction steps, expected vs actual behavior |
| **âœ¨ New Features** | Discuss in issues first, include tests and documentation |
| **ğŸ“š Documentation** | Clear examples, practical use cases, proper formatting |
| **ğŸ”§ Performance** | Benchmark results, before/after comparisons |

### ğŸ† Recognition

Contributors will be recognized in:
- ğŸ“ Release notes for significant contributions
- ğŸŒŸ Contributors section (with permission)
- ğŸ’¬ Community showcase for innovative use cases

---

## ğŸ“ Support & Resources

### ğŸ’¬ Getting Help

| Resource | Description | Link |
|----------|-------------|------|
| **ğŸ“š Documentation** | Complete setup and usage guides | [docs/](docs/) |
| **ï¿½ğŸ› Issue Tracker** | Bug reports and feature requests | [GitHub Issues](https://github.com/Azure-Samples/ARGUS/issues) |
| **ğŸ’¡ Discussions** | Community Q&A and ideas | [GitHub Discussions](https://github.com/Azure-Samples/ARGUS/discussions) |
| **ğŸ“§ Team Contact** | Direct contact for enterprise needs | See team section below |

### ğŸ”— Additional Resources

- **ğŸ“– Azure Document Intelligence**: [Official Documentation](https://docs.microsoft.com/azure/applied-ai-services/form-recognizer/)
- **ğŸ¤– Azure OpenAI**: [Service Documentation](https://docs.microsoft.com/azure/cognitive-services/openai/)
- **âš¡ FastAPI**: [Framework Documentation](https://fastapi.tiangolo.com/)
- **ğŸ LangChain**: [Integration Guides](https://python.langchain.com/)

---

## ğŸ‘¥ Team

- **Alberto Gallo**
- **Petteri Johansson**
- **Christin Pohl**
- **Konstantinos Mavrodis**

## License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

## ğŸš€ Ready to Transform Your Document Processing?

**Deploy ARGUS in minutes and start extracting intelligence from your documents today!**

```bash
git clone https://github.com/Azure-Samples/ARGUS.git && cd ARGUS && azd up
```

<br>

[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template)
[![Open in Dev Container](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/Azure-Samples/ARGUS)

<br>

**â­ Star this repo if ARGUS helps your document processing needs!**

</div>
