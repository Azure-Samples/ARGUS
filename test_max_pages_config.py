#!/usr/bin/env python3
"""
Test script to validate the max_pages_per_chunk configuration changes
"""

import json
from datetime import datetime

# Test the configuration data structure
def test_dataset_config():
    """Test that dataset configuration includes max_pages_per_chunk"""
    
    # Test configuration data as it would appear in Cosmos DB
    config_data = {
        "id": "configuration",
        "partitionKey": "configuration",
        "datasets": {
            "default-dataset": {
                "model_prompt": "Extract all data.",
                "example_schema": {"field1": "value1"},
                "max_pages_per_chunk": 10
            },
            "medical-dataset": {
                "model_prompt": "Extract medical data.",
                "example_schema": {"patient": "name"},
                "max_pages_per_chunk": 5
            }
        }
    }
    
    # Test fetching max_pages_per_chunk
    default_max_pages = config_data["datasets"]["default-dataset"].get("max_pages_per_chunk", 10)
    medical_max_pages = config_data["datasets"]["medical-dataset"].get("max_pages_per_chunk", 10)
    
    print(f"Default dataset max pages per chunk: {default_max_pages}")
    print(f"Medical dataset max pages per chunk: {medical_max_pages}")
    
    # Test backward compatibility - dataset without max_pages_per_chunk
    legacy_dataset = {
        "model_prompt": "Legacy prompt",
        "example_schema": {"legacy": "field"}
    }
    legacy_max_pages = legacy_dataset.get("max_pages_per_chunk", 10)
    print(f"Legacy dataset max pages per chunk (default): {legacy_max_pages}")
    
    assert default_max_pages == 10, "Default dataset should have 10 pages per chunk"
    assert medical_max_pages == 5, "Medical dataset should have 5 pages per chunk"
    assert legacy_max_pages == 10, "Legacy dataset should default to 10 pages per chunk"
    
    print("âœ… All configuration tests passed!")

def test_document_structure():
    """Test the document structure includes max_pages_per_chunk"""
    
    # Simulate the document structure that would be created
    document = {
        "id": "test-document",
        "dataset": "default-dataset",
        "properties": {
            "blob_name": "default-dataset/test.pdf",
            "blob_size": 1024,
            "request_timestamp": datetime.now().isoformat(),
            "num_pages": 25,
            "dataset": "default-dataset"
        },
        "model_input": {
            "model_deployment": "gpt-4",
            "model_prompt": "Extract all data.",
            "example_schema": {"field1": "value1"},
            "max_pages_per_chunk": 10
        }
    }
    
    # Test that max_pages_per_chunk is accessible
    max_pages = document["model_input"].get("max_pages_per_chunk", 10)
    print(f"Document max pages per chunk: {max_pages}")
    
    # Test splitting logic
    num_pages = document["properties"]["num_pages"]
    should_split = num_pages > max_pages
    print(f"Document has {num_pages} pages, should split: {should_split}")
    
    if should_split:
        num_chunks = (num_pages + max_pages - 1) // max_pages  # Ceiling division
        print(f"Document would be split into {num_chunks} chunks")
    
    assert max_pages == 10, "Document should have 10 as max pages per chunk"
    assert should_split == True, "Document with 25 pages should be split with max 10 pages per chunk"
    
    print("âœ… All document structure tests passed!")

if __name__ == "__main__":
    print("Testing max_pages_per_chunk configuration changes...")
    print("=" * 60)
    
    test_dataset_config()
    print()
    test_document_structure()
    
    print()
    print("ðŸŽ‰ All tests completed successfully!")
    print("The max_pages_per_chunk configuration feature is ready for deployment.")
